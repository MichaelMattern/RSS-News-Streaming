# Finance News RSSÂ Scraper

Collect the latest business and markets headlines from a handful of handâ€‘picked RSS feeds and store them in a single compressed [Parquet](https://parquet.apache.org/) file for fast, columnâ€‘oriented analytics.

---

## âœ¨Â Key Features

| Feature                  | Details                                                                                                                                     |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Parallel downloads**   | Each RSS feed is fetched concurrently with `ThreadPoolExecutor`, making a full refresh finish in only a few seconds.                        |
| **Canonical schema**     | Every entry is normalized into the same column setÂ â€” `article_id`, `timestamp`, `title`, `summary`, `tags`, `authors`, `url`, and `source`. |
| **Stable IDs**           | A SHAâ€‘1 hash of the article URL yields a 40â€‘character key that survives feed reorderings and duplicates.                                    |
| **Timezone handling**    | Timestamps are converted to America/New\_York (`NY_TZ`) and saved in ISOâ€‘8601 format.                                                       |
| **Zeroâ€‘copy storage**    | Output is written to `news.parquet` using the ultraâ€‘fast **pyarrow** engine with **zstd** compression.                                      |
| **Simple extensibility** | Add or remove feeds by editing the `SOURCES` list at the bottom of the script.                                                              |

---

## ğŸ”§Â Requirements

- **PythonÂ â‰¥Â 3.9** (for the `zoneinfo` module)
- pip packages:
  ```bash
  pip install feedparser beautifulsoup4 pandas pyarrow
  ```

> **TipÂ ğŸ’¡**â€ƒCreate an isolated environment first:
>
> ```bash
> python -m venv .venv && source .venv/bin/activate  # PowerShell: .venv\Scripts\Activate.ps1
> ```

---

## ğŸš€Â Running the Scraper

```bash
python rss_scraper.py
```

Sample output:

```
âœ… Parquet written: 1.83Â MB (162Â articles)
â±ï¸  Total run time: 4.27Â s
â±ï¸  Per-article time: 0.03Â s
```

The Parquet file is created in the working directory and can be inspected with `pandas`:

```python
import pandas as pd
df = pd.read_parquet("news.parquet")
print(df.head())
```

---

## ğŸ“Â Customizing the Feed List

Open `rss_scraper.py` and edit the `SOURCES` list:

```python
SOURCES = [
    "http://rss.nytimes.com/services/xml/rss/nyt/Business.xml",
    # add more â€¦
]
```

> **Good practice:** keep the list short and relevant â€” many generalâ€‘news feeds publish hundreds of unrelated stories per hour.

---

## âš™ï¸Â How It Works (Under the Hood)

1. **Fetch & parse**â€ƒAll feeds are downloaded in parallel (max 16 workers) and processed by `feedparser`.
2. **Canonicalise**â€ƒEach entry is fed to `canonical_row()` where:
   - The publication date is parsed to UTC, converted to `NY_TZ`, and ISOâ€‘formatted.
   - The article URL is hashed (`sha1`) to produce a unique `article_id`.
   - HTML summaries are stripped to plain text with **BeautifulSoup**.
3. **Frame & write**â€ƒRows are collected into a `pandas.DataFrame`, duplicates are dropped, and the table is sorted newestâ€‘first before being persisted as Parquet with **Zstandard** compression.

---

## ğŸ”Â Querying the Output (Examples)

```python
# Top publishers in the last 24Â h
(df[df.timestamp > (pd.Timestamp.utcnow() - pd.Timedelta('1D')).isoformat()]
   .groupby('source').size().sort_values(ascending=False).head())
```

```python
# Search headlines for a ticker symbol
mask = df.title.str.contains(r"\bMSFT\b", case=False, regex=True)
print(df.loc[mask, ["timestamp", "title", "url"]])
```

---

## ğŸï¸Â Performance Tweaks

| Parameter           | Where                                 | Why you might change it                                                                  |
| ------------------- | ------------------------------------- | ---------------------------------------------------------------------------------------- |
| `max_feed_workers`  | topâ€‘level `__main__` block            | Increase for *many* feeds; decrease on lowâ€‘core machines or to be gentle on the sources. |
| `max_row_workers`   | same                                  | Controls how many entries are canonicalised in parallel.                                 |
| Parquet compression | `to_parquet(..., compression="zstd")` | Switch to `snappy`, `gzip`, or leave `None` for speed over size.                         |

---

## ğŸ› ï¸Â Extending the Schema

Need more columns?Â Inside `canonical_row()` you already have access to the raw `feed_entry` object. Add new keys to the returned dict and they will appear automatically in the DataFrame.


